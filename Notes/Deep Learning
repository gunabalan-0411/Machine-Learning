# Neural Network (Fundamental)

## Vanilla Neural Network

### Input layer
* Neuron: Thing that holds a number [0-1]
* So 28*28 input image when passed to NN has 784 (28*28) neurons
* here each neuron represents the grayscale value (0-black, 1-white)
* This number inside the neuron is called its activation.

### Hidden layer
* Here the number of layors and number of neuron in each layour is Arbitrary(selected for convenience, convention, or practicality, not because it is the only correct option) choice

### Output Layor


* The way the network operates, Activation in one neuron layer determines the activation in other layer till output layer
* So how does one layer influence the next? or How does trainig work?

Let's say we have two hidden layer

* The neurons in hidden layer *2* has some numbers representing various shapes (o, /, |, -) with which each digits can be formed if selected rightly.
* So how these shapes are calculated and stored in each neuron in hidden layer *2*?
* Ex: recognizing a loop or circle shape is a sub-problem, in first we find out the edges.
* The neurons in hidden layer *1* has some numbers that represents the edges of these shapes

So as a whole we need a mechanism that converts these pixels to edges to shapes/ pattern to digits.
* if we multiple weights to each 786 neurons and by assuming some pixels becomes 0 and some become 1 so form an edge.
* Now from input layer all the summation of activations * weights (w1a1 + w2a2 ... w784a784) to get a weighted sum.
* but we need the activations to be in between 0 to 1, so we use sigmoid function to get that. (1/1+e^-z), z is linear equation.
* neg values towards 0 and positive values towards 1, and after input 0 the sigmoid value will be getting high (look at the graph)
* Meaning so basically the activition here is how positive is this?/ how positive to relevant weighted sum.
* But if we don't want the weighted sum to be increased after 0, but activate meaningfully when input cross 10, then we can subtract a *bias* (like 10) from weighted sum before sigmoid

* Now all the above is for just one neuron in first hidden layer.
* Now each neuron from hidden layer connect to all 786 neuron and each has own weights and bias.

* The pattern like weight sum, bias repeated for subsequent layers.

* *Learning* is actually finding the right weights and bias for all neurons


#### How the calculation happen
* Matrix vector product (multiple and sum)
- [w0_1, w0_2, ... w0_n]   [a0]    [?]    [b1]
- [     ,    .  ,      ] * [..] =  [..] + [..]   = a(1) = 	σ(Wa + b)
- [wk_1, wk_2, ... wk_n]   [an]    [?]    [bn]

* This matrix multiplication helps coding to solve it faster.
* So each neuron in hidden layer is kind of a function which inputs numbers and outputs number
* similarly entire neural network

* [Note] Modern Neural network using ReLu(max(0, a)) than sigmoid

### Gradient descent
* Initially we randomize those 786 * 16 + 16 * 16 + 16 * 10 + 16 * 2 = ~13K parameters 
* So we first calculate the cost (predicated - actual), if its more then its right parameter or bad parameter value
* We get the average of cost of all the input images, to check how lousy the network is or not.
* So if we want randomly assign some numbers and getting the cost value towards is 0 is extremely computational
* Gradient descent method is an optimized way to do that
* In a space of weight and cost value (a wave like graph will be formed with various global minimum).
* So if we find the lowest possible global minimum where cost is  lowest and weight is lowest that is optimal weights.
* But which direction we move in the waving graph?
  * By calculating slope, if slope is positive shift to left, negative shift to right (basically move to opposite direction of slope)
  * The image we can imagine is like rolling down a hill.
* But there are many possible valleys found in that hill.
* using calculus further to find the steepest hill and then move to the direction of global minimum
* The negative gradient tells you how to change the weights and biases so the error goes down as fast as possible.
* What is a gradient? At your current position, ask this question: “If I slightly change each weight and bias, which direction makes the cost increase the fastest?”
* Why negative gradient? Negative gradient = steepest downhill direction

### Backpropagation (How neural network learn)
* Backpropagation is the algorithm helps to calculate the gradient we talk above.
* From the last layer we note the list desired changes of weights & bias to choose a particular target variable.
* Now in second to last layour we do the same by note the desired changes to choose individual neuron in last layer.
* this goes up till earlier neuron, by moving backwards.
* by iterating all example data we will get an average weights for each target variable
* and this average weights is proportional to negative gradient.
* And instead of calculating this step (negative gradient) for all sample data. If we split into mini batches and the find the steps accordingly, it will be easy to find the next optimal step/ direction, it called stochatic Gradient descent.
* this looks like a plot of stubmled direction but even if its stubmled its faster to reach the global minimum.

* Backpropagation uses the chain rule to figure out how a small change in each weight affects the final error, step by step, backwards.
* Calculus is just a tool to measure sensitivity. Change something a little Observe how much the output changes.
* That “how much” is what calculus measures.
* A derivative simply means: “If I nudge this value a tiny bit, how much does the result change?”
* A neural network is not one step, So the weight does not directly affect the loss. It affects something → which affects something → which affects something → which affects loss.
* effect of A on B × effect of B on C × effect of C on brightness. That multiplication is the chain rule.
* Why backpropagation? Each step: assigns blame tells how responsible each weight was, This backward blame assignment uses the chain rule. This is why we need the chain rule.
* Calculus tells us how sensitive things are. The chain rule tells us how sensitivity flows through a chain. Backprop uses the chain rule to send error information backward and adjust each weight correctly.



Easy intuition when a 786 neuron passed it first light up the edges in hidden layer one and the pattern in layer 2 and then the digits.






* [NOTE] The Transformer MLP is a vanilla feed-forward neural network mathematically, but it plays a very specialized, position-wise role and is not equivalent to a standalone vanilla MLP model.



# CNN (Convolutional Neural Network) — How It Works + Architecture (Easy + In-Depth)

This canvas explains CNNs from the ground up:

* why CNN is needed
* convolution operation
* filters/kernels
* feature maps
* padding/stride
* pooling
* full CNN architecture
* training basics
* popular CNN families
* interview questions

---

# 1) Why do we need CNN?

## Problem with normal neural networks for images

Images have 2D structure.
Example: 28×28 image → 784 pixels.

If we use a fully connected neural network:

* input layer has 784 neurons
* each neuron connects to all hidden neurons

### Issues

1. Too many parameters
2. Overfitting risk
3. Doesn’t exploit local patterns (edges, corners)

---

# 2) Core idea of CNN

CNN learns **local features** using small filters.

Instead of connecting everything to everything:
✅ CNN connects each neuron only to a local region.

### Example intuition

To recognize a face:

* first detect edges
* then eyes/nose
* then whole face

CNN learns features in this hierarchy.

---

# 3) Convolution Operation (most important)

## 3.1 What is a convolution filter / kernel?

A kernel is a small matrix (like 3×3).

Example kernel:

```
[ 1  0 -1
  1  0 -1
  1  0 -1 ]
```

This detects vertical edges.

---

## 3.2 How convolution works

You slide the kernel across the image.

At each location:

* multiply kernel values with pixel values
* sum them

This produces **one output pixel**.

That output grid is called a **feature map**.

---

## 3.3 Why it works

Because kernel learns patterns:

* edge detector
* curve detector
* texture detector

And those patterns repeat in different image locations.

So CNN shares weights across image.

✅ This is called **parameter sharing**.

---

# 4) Feature maps and channels

## 4.1 Channels

A color image has 3 channels:

* R, G, B

So input shape:

* `(H, W, 3)`

A convolution kernel for RGB has depth 3:

* `(kH, kW, 3)`

---

## 4.2 Multiple filters

CNN uses many kernels.

If we use 64 kernels:

* output will have 64 feature maps

So output shape:

* `(H_out, W_out, 64)`

Each filter learns a different pattern.

---

# 5) Stride and Padding

## 5.1 Stride

Stride = step size while moving kernel.

* stride=1 → moves 1 pixel at a time
* stride=2 → moves 2 pixels

Higher stride:
✅ reduces output size (downsampling)

---

## 5.2 Padding

Padding adds extra border around image.

Types:

### Valid padding

* no padding
* output shrinks

### Same padding

* pad so output has same height/width

Why padding is needed?

* preserve edge information
* control output size

---

## 5.3 Output size formula

For 1 dimension:

`out = floor((n + 2p - f)/s) + 1`

Where:

* n = input size
* p = padding
* f = filter size
* s = stride

---

# 6) Activation function

After convolution, we apply non-linearity.

Most common:

* ReLU

`ReLU(x) = max(0, x)`

Why?

* helps learn complex patterns

---

# 7) Pooling layer

Pooling reduces spatial size.

## 7.1 Max Pooling (common)

Example: 2×2 pooling

* choose maximum value

Benefits:
✅ reduces computation
✅ makes model robust to small shifts

---

## 7.2 Average Pooling

* takes average

Used sometimes.

---

# 8) Typical CNN Architecture

### Standard CNN pipeline

```
Input Image
  ↓
[Conv → ReLU → Pool]
  ↓
[Conv → ReLU → Pool]
  ↓
[Conv → ReLU]
  ↓
Flatten
  ↓
Dense / Fully connected
  ↓
Softmax / Sigmoid output
```

---

# 9) What does each stage learn?

## Early layers

* edges
* gradients

## Middle layers

* textures
* corners

## Deep layers

* object parts
* whole object concept

---

# 10) CNN Training Basics

Training is same as other NNs:

1. forward pass
2. compute loss
3. backprop
4. update weights

Loss functions:

* classification: cross entropy
* regression: MSE

Optimizers:

* SGD
* Adam

---

# 11) CNN Special Concepts

## 11.1 Receptive field

Receptive field = region of input affecting a neuron.

As layers go deeper:

* receptive field increases

So deep neurons "see" larger part of image.

---

## 11.2 Translation invariance

CNN becomes robust to shifting object slightly.

Pooling helps.

---

## 11.3 Batch Normalization

Normalizes activations.

Benefits:

* faster training
* stable gradients

---

## 11.4 Dropout

Regularization to reduce overfitting.

---

# 12) Popular CNN Architectures (must know)

## LeNet

Early CNN for digits.

## AlexNet

Started deep learning boom.

## VGG

Very deep with 3×3 convs.

## ResNet

Key idea: **skip connections**
Solves vanishing gradients.

## Inception

Multi-scale filters in parallel.

## MobileNet

Efficient for mobile.
Uses depthwise separable conv.

---

# 13) CNN vs Transformer for vision

### CNN

* local inductive bias
* efficient for small datasets

### Vision Transformer (ViT)

* attention-based
* needs large data
* captures global context

---

# 14) Tricky Interview Questions

## Q1) Why CNN has fewer parameters than Dense NN?

Because:

* local connectivity
* weight sharing

---

## Q2) What does padding do?

* controls output size
* preserves edge pixels

---

## Q3) Why max pooling?

* downsample
* invariance
* reduce computation

---

## Q4) What is receptive field?

* part of input influencing neuron

---

## Q5) Why ResNet is important?

Skip connection improves gradient flow.

---

# 15) Summary (1-minute)

* CNN learns image features using convolution filters
* convolution produces feature maps
* multiple filters learn multiple patterns
* padding/stride control output shape
* pooling reduces dimension
* deeper layers learn higher-level features

---
