# Neural Network

## Vanilla Neural Network

### Input layer
* Neuron: Thing that holds a number [0-1]
* So 28*28 input image when passed to NN has 784 (28*28) neurons
* here each neuron represents the grayscale value (0-black, 1-white)
* This number inside the neuron is called its activation.

### Hidden layer
* Here the number of layors and number of neuron in each layour is Arbitrary(selected for convenience, convention, or practicality, not because it is the only correct option) choice

### Output Layor


* The way the network operates, Activation in one neuron layer determines the activation in other layer till output layer
* So how does one layer influence the next? or How does trainig work?

Let's say we have two hidden layer

* The neurons in hidden layer *2* has some numbers representing various shapes (o, /, |, -) with which each digits can be formed if selected rightly.
* So how these shapes are calculated and stored in each neuron in hidden layer *2*?
* Ex: recognizing a loop or circle shape is a sub-problem, in first we find out the edges.
* The neurons in hidden layer *1* has some numbers that represents the edges of these shapes

So as a whole we need a mechanism that converts these pixels to edges to shapes/ pattern to digits.
* Now all the summation of activatios + weights (w1a1 + w2a2 ... w784a784)





* [NOTE] The Transformer MLP is a vanilla feed-forward neural network mathematically, but it plays a very specialized, position-wise role and is not equivalent to a standalone vanilla MLP model.



# CNN (Convolutional Neural Network) — How It Works + Architecture (Easy + In-Depth)

This canvas explains CNNs from the ground up:

* why CNN is needed
* convolution operation
* filters/kernels
* feature maps
* padding/stride
* pooling
* full CNN architecture
* training basics
* popular CNN families
* interview questions

---

# 1) Why do we need CNN?

## Problem with normal neural networks for images

Images have 2D structure.
Example: 28×28 image → 784 pixels.

If we use a fully connected neural network:

* input layer has 784 neurons
* each neuron connects to all hidden neurons

### Issues

1. Too many parameters
2. Overfitting risk
3. Doesn’t exploit local patterns (edges, corners)

---

# 2) Core idea of CNN

CNN learns **local features** using small filters.

Instead of connecting everything to everything:
✅ CNN connects each neuron only to a local region.

### Example intuition

To recognize a face:

* first detect edges
* then eyes/nose
* then whole face

CNN learns features in this hierarchy.

---

# 3) Convolution Operation (most important)

## 3.1 What is a convolution filter / kernel?

A kernel is a small matrix (like 3×3).

Example kernel:

```
[ 1  0 -1
  1  0 -1
  1  0 -1 ]
```

This detects vertical edges.

---

## 3.2 How convolution works

You slide the kernel across the image.

At each location:

* multiply kernel values with pixel values
* sum them

This produces **one output pixel**.

That output grid is called a **feature map**.

---

## 3.3 Why it works

Because kernel learns patterns:

* edge detector
* curve detector
* texture detector

And those patterns repeat in different image locations.

So CNN shares weights across image.

✅ This is called **parameter sharing**.

---

# 4) Feature maps and channels

## 4.1 Channels

A color image has 3 channels:

* R, G, B

So input shape:

* `(H, W, 3)`

A convolution kernel for RGB has depth 3:

* `(kH, kW, 3)`

---

## 4.2 Multiple filters

CNN uses many kernels.

If we use 64 kernels:

* output will have 64 feature maps

So output shape:

* `(H_out, W_out, 64)`

Each filter learns a different pattern.

---

# 5) Stride and Padding

## 5.1 Stride

Stride = step size while moving kernel.

* stride=1 → moves 1 pixel at a time
* stride=2 → moves 2 pixels

Higher stride:
✅ reduces output size (downsampling)

---

## 5.2 Padding

Padding adds extra border around image.

Types:

### Valid padding

* no padding
* output shrinks

### Same padding

* pad so output has same height/width

Why padding is needed?

* preserve edge information
* control output size

---

## 5.3 Output size formula

For 1 dimension:

`out = floor((n + 2p - f)/s) + 1`

Where:

* n = input size
* p = padding
* f = filter size
* s = stride

---

# 6) Activation function

After convolution, we apply non-linearity.

Most common:

* ReLU

`ReLU(x) = max(0, x)`

Why?

* helps learn complex patterns

---

# 7) Pooling layer

Pooling reduces spatial size.

## 7.1 Max Pooling (common)

Example: 2×2 pooling

* choose maximum value

Benefits:
✅ reduces computation
✅ makes model robust to small shifts

---

## 7.2 Average Pooling

* takes average

Used sometimes.

---

# 8) Typical CNN Architecture

### Standard CNN pipeline

```
Input Image
  ↓
[Conv → ReLU → Pool]
  ↓
[Conv → ReLU → Pool]
  ↓
[Conv → ReLU]
  ↓
Flatten
  ↓
Dense / Fully connected
  ↓
Softmax / Sigmoid output
```

---

# 9) What does each stage learn?

## Early layers

* edges
* gradients

## Middle layers

* textures
* corners

## Deep layers

* object parts
* whole object concept

---

# 10) CNN Training Basics

Training is same as other NNs:

1. forward pass
2. compute loss
3. backprop
4. update weights

Loss functions:

* classification: cross entropy
* regression: MSE

Optimizers:

* SGD
* Adam

---

# 11) CNN Special Concepts

## 11.1 Receptive field

Receptive field = region of input affecting a neuron.

As layers go deeper:

* receptive field increases

So deep neurons "see" larger part of image.

---

## 11.2 Translation invariance

CNN becomes robust to shifting object slightly.

Pooling helps.

---

## 11.3 Batch Normalization

Normalizes activations.

Benefits:

* faster training
* stable gradients

---

## 11.4 Dropout

Regularization to reduce overfitting.

---

# 12) Popular CNN Architectures (must know)

## LeNet

Early CNN for digits.

## AlexNet

Started deep learning boom.

## VGG

Very deep with 3×3 convs.

## ResNet

Key idea: **skip connections**
Solves vanishing gradients.

## Inception

Multi-scale filters in parallel.

## MobileNet

Efficient for mobile.
Uses depthwise separable conv.

---

# 13) CNN vs Transformer for vision

### CNN

* local inductive bias
* efficient for small datasets

### Vision Transformer (ViT)

* attention-based
* needs large data
* captures global context

---

# 14) Tricky Interview Questions

## Q1) Why CNN has fewer parameters than Dense NN?

Because:

* local connectivity
* weight sharing

---

## Q2) What does padding do?

* controls output size
* preserves edge pixels

---

## Q3) Why max pooling?

* downsample
* invariance
* reduce computation

---

## Q4) What is receptive field?

* part of input influencing neuron

---

## Q5) Why ResNet is important?

Skip connection improves gradient flow.

---

# 15) Summary (1-minute)

* CNN learns image features using convolution filters
* convolution produces feature maps
* multiple filters learn multiple patterns
* padding/stride control output shape
* pooling reduces dimension
* deeper layers learn higher-level features

---
