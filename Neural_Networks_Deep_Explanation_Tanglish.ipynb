{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - Complete Deep Explanation (Tanglish)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Indha notebook la namma **Neural Networks** pathi complete ah, in-depth ah, formulas oda purinjikka porom. Rendu examples use pannuvom:\n",
    "\n",
    "1. **Regression Problem** - House Price Prediction\n",
    "2. **Classification Problem** - Binary Classification\n",
    "\n",
    "Yellam step-by-step ah, formula oda, clear ah explain pannuvom!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: REGRESSION EXAMPLE\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Dataset:**\n",
    "- **Input (X):** Size (sq.ft) = [1000, 1500, 2000]\n",
    "- **Output (Y):** Price (₹ lakhs) = [50, 70, 90]\n",
    "\n",
    "**Goal:** Veedu size kuduthu, price predict pannanum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Input Layer - Data Preparation\n",
    "\n",
    "### 1.1 Raw Data\n",
    "\n",
    "Namma kitta irukura raw data:\n",
    "\n",
    "```\n",
    "Size (sq.ft): [1000, 1500, 2000]\n",
    "Price (₹ lakhs): [50, 70, 90]\n",
    "```\n",
    "\n",
    "### 1.2 Input ah Vector ah Convert Panradhu\n",
    "\n",
    "**Question:** Inputs la yepadi vector ah change panranga?\n",
    "\n",
    "**Answer:** Neural network ku data matrix format la (2D array) kudukanum. Indha process ku **Feature Matrix** nu solluvom.\n",
    "\n",
    "**Original Data:**\n",
    "```\n",
    "X = [1000, 1500, 2000]  → Idhu 1D array (list)\n",
    "```\n",
    "\n",
    "**Vector Format (Matrix):**\n",
    "```\n",
    "X = [[1000],\n",
    "     [1500],\n",
    "     [2000]]\n",
    "```\n",
    "\n",
    "Idhu oru **3×1 matrix** (3 samples, 1 feature).\n",
    "\n",
    "**Mathematical Representation:**\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x^{(1)} \\\\\n",
    "x^{(2)} \\\\\n",
    "x^{(3)}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1000 \\\\\n",
    "1500 \\\\\n",
    "2000\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Inga:\n",
    "- $x^{(1)}$ = First training example (1000 sq.ft)\n",
    "- $x^{(2)}$ = Second training example (1500 sq.ft)\n",
    "- $x^{(3)}$ = Third training example (2000 sq.ft)\n",
    "\n",
    "### 1.3 Normalization (Optional but Important)\n",
    "\n",
    "Large values irundha training slow ah irukkum. Adhunala normalize pannuvom:\n",
    "\n",
    "**Min-Max Normalization Formula:**\n",
    "\n",
    "$$\n",
    "X_{normalized} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "$$\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "$$\n",
    "X_{normalized} = \\frac{[1000, 1500, 2000] - 1000}{2000 - 1000} = \\frac{[0, 500, 1000]}{1000} = [0, 0.5, 1.0]\n",
    "$$\n",
    "\n",
    "**Final Input Matrix:**\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.5 \\\\\n",
    "1.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similarly, output Y um normalize pannalam:\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "50 \\\\\n",
    "70 \\\\\n",
    "90\n",
    "\\end{bmatrix} \\rightarrow Y_{normalized} = \\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.5 \\\\\n",
    "1.0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Network Architecture Design\n",
    "\n",
    "Namma oru simple neural network design pannuvom:\n",
    "\n",
    "**Architecture:**\n",
    "- **Input Layer:** 1 neuron (Size feature)\n",
    "- **Hidden Layer 1:** 3 neurons\n",
    "- **Output Layer:** 1 neuron (Price prediction)\n",
    "\n",
    "**Visual Representation:**\n",
    "\n",
    "```\n",
    "Input Layer    Hidden Layer    Output Layer\n",
    "    (1)            (3)              (1)\n",
    "    \n",
    "    [X] ---------> [H1] \n",
    "                   [H2] ---------> [Y]\n",
    "                   [H3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Weight Initialization\n",
    "\n",
    "### 3.1 Weights Yepadi Initialize Panranga?\n",
    "\n",
    "**Question:** Inputs la irunthu weights yepadi initialize panranga?\n",
    "\n",
    "**Answer:** Weights ah **random ah** initialize pannuvom, but small values la. Idhu training ku help pannum.\n",
    "\n",
    "### 3.2 Weight Matrices\n",
    "\n",
    "**Between Input and Hidden Layer:**\n",
    "\n",
    "Weight matrix $W^{[1]}$ size: **(number of hidden neurons) × (number of input features)**\n",
    "\n",
    "$$\n",
    "W^{[1]} = \\begin{bmatrix}\n",
    "w_{11}^{[1]} \\\\\n",
    "w_{21}^{[1]} \\\\\n",
    "w_{31}^{[1]}\n",
    "\\end{bmatrix} \\text{ (3×1 matrix)}\n",
    "$$\n",
    "\n",
    "**Example Random Initialization:**\n",
    "\n",
    "$$\n",
    "W^{[1]} = \\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "-0.3 \\\\\n",
    "0.8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Bias for Hidden Layer:**\n",
    "\n",
    "$$\n",
    "b^{[1]} = \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix} \\text{ (3×1 matrix)}\n",
    "$$\n",
    "\n",
    "**Between Hidden and Output Layer:**\n",
    "\n",
    "Weight matrix $W^{[2]}$ size: **(number of output neurons) × (number of hidden neurons)**\n",
    "\n",
    "$$\n",
    "W^{[2]} = \\begin{bmatrix}\n",
    "w_{11}^{[2]} & w_{12}^{[2]} & w_{13}^{[2]}\n",
    "\\end{bmatrix} \\text{ (1×3 matrix)}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "W^{[2]} = \\begin{bmatrix}\n",
    "0.4 & 0.6 & -0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Bias for Output Layer:**\n",
    "\n",
    "$$\n",
    "b^{[2]} = [0.3] \\text{ (scalar)}\n",
    "$$\n",
    "\n",
    "### 3.3 Why Random Initialization?\n",
    "\n",
    "- Ella weights um same value la initialize pannina, ella neurons um same ah learn pannudum (symmetry problem)\n",
    "- Random values kuduthu, each neuron different patterns learn pannum\n",
    "- Small values use panradhu gradient explosion avoid panna help pannum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Forward Propagation\n",
    "\n",
    "### 4.1 Input to Hidden Layer\n",
    "\n",
    "**Question:** Weights and multiplication yepadi nadakuthu?\n",
    "\n",
    "**Answer:** Matrix multiplication use panni, weighted sum calculate pannuvom.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "Z^{[1]} = W^{[1]} \\cdot X + b^{[1]}\n",
    "$$\n",
    "\n",
    "Inga:\n",
    "- $Z^{[1]}$ = Pre-activation values (hidden layer ku)\n",
    "- $W^{[1]}$ = Weights (3×1)\n",
    "- $X$ = Input (1×1 for one sample)\n",
    "- $b^{[1]}$ = Bias (3×1)\n",
    "\n",
    "**First Training Example Calculation ($x^{(1)} = 0.0$):**\n",
    "\n",
    "$$\n",
    "Z^{[1]} = \\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "-0.3 \\\\\n",
    "0.8\n",
    "\\end{bmatrix} \\times 0.0 + \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.0 + 0.1 \\\\\n",
    "0.0 + 0.2 \\\\\n",
    "0.0 - 0.1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Detailed Breakdown:**\n",
    "\n",
    "Each hidden neuron ku:\n",
    "- $z_1^{[1]} = w_{11}^{[1]} \\times x + b_1^{[1]} = 0.5 \\times 0.0 + 0.1 = 0.1$\n",
    "- $z_2^{[1]} = w_{21}^{[1]} \\times x + b_2^{[1]} = -0.3 \\times 0.0 + 0.2 = 0.2$\n",
    "- $z_3^{[1]} = w_{31}^{[1]} \\times x + b_3^{[1]} = 0.8 \\times 0.0 - 0.1 = -0.1$\n",
    "\n",
    "### 4.2 Activation Function (Hidden Layer)\n",
    "\n",
    "**Question:** Activation nadakalana yena agum?\n",
    "\n",
    "**Answer:** Activation function illaama, neural network oru simple linear regression dhan agidum. Non-linearity add panna dhan complex patterns learn panna mudiyum.\n",
    "\n",
    "**Common Activation: ReLU (Rectified Linear Unit)**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z) = \\begin{cases}\n",
    "z & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Apply ReLU:**\n",
    "\n",
    "$$\n",
    "A^{[1]} = \\text{ReLU}(Z^{[1]}) = \\begin{bmatrix}\n",
    "\\max(0, 0.1) \\\\\n",
    "\\max(0, 0.2) \\\\\n",
    "\\max(0, -0.1)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Why ReLU?**\n",
    "- Simple and fast to compute\n",
    "- Negative values ah 0 aakidum\n",
    "- Positive values ah as-is pass pannum\n",
    "- Gradient vanishing problem ah reduce pannum\n",
    "\n",
    "### 4.3 Hidden to Output Layer\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "Z^{[2]} = W^{[2]} \\cdot A^{[1]} + b^{[2]}\n",
    "$$\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "$$\n",
    "Z^{[2]} = \\begin{bmatrix}\n",
    "0.4 & 0.6 & -0.2\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2 \\\\\n",
    "0.0\n",
    "\\end{bmatrix} + 0.3\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{[2]} = (0.4 \\times 0.1) + (0.6 \\times 0.2) + (-0.2 \\times 0.0) + 0.3\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{[2]} = 0.04 + 0.12 + 0.0 + 0.3 = 0.46\n",
    "$$\n",
    "\n",
    "### 4.4 Output Activation (Regression)\n",
    "\n",
    "Regression ku, output layer la **Linear Activation** (or no activation) use pannuvom:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = A^{[2]} = Z^{[2]} = 0.46\n",
    "$$\n",
    "\n",
    "Idhu namma **predicted price** (normalized form la)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Loss Calculation\n",
    "\n",
    "### 5.1 Loss Function for Regression\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2}(Y - \\hat{Y})^2\n",
    "$$\n",
    "\n",
    "Inga:\n",
    "- $Y$ = Actual value\n",
    "- $\\hat{Y}$ = Predicted value\n",
    "- $\\frac{1}{2}$ = Mathematical convenience ku (derivative easy aagum)\n",
    "\n",
    "**For First Sample:**\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2}(0.0 - 0.46)^2 = \\frac{1}{2}(-0.46)^2 = \\frac{1}{2}(0.2116) = 0.1058\n",
    "$$\n",
    "\n",
    "### 5.2 Total Loss (All Samples)\n",
    "\n",
    "Ella samples kum loss calculate panni average edukanum:\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{m} \\sum_{i=1}^{m} L^{(i)} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{1}{2}(y^{(i)} - \\hat{y}^{(i)})^2\n",
    "$$\n",
    "\n",
    "Inga $m$ = number of training examples (3 in our case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Backpropagation\n",
    "\n",
    "### 6.1 What is Backpropagation?\n",
    "\n",
    "Loss ah minimize panna, weights ah adjust pannanum. Backpropagation use panni, each weight ku **gradient** (slope) calculate pannuvom.\n",
    "\n",
    "**Chain Rule:**\n",
    "\n",
    "Calculus la chain rule use panni, loss ah each weight respect ah differentiate pannuvom.\n",
    "\n",
    "### 6.2 Output Layer Gradient\n",
    "\n",
    "**Loss respect to output:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{Y}} = \\frac{\\partial}{\\partial \\hat{Y}} \\left[ \\frac{1}{2}(Y - \\hat{Y})^2 \\right] = -(Y - \\hat{Y})\n",
    "$$\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{Y}} = -(0.0 - 0.46) = 0.46\n",
    "$$\n",
    "\n",
    "**Output activation gradient (Linear):**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{Y}}{\\partial Z^{[2]}} = 1\n",
    "$$\n",
    "\n",
    "**Combined:**\n",
    "\n",
    "$$\n",
    "dZ^{[2]} = \\frac{\\partial L}{\\partial Z^{[2]}} = \\frac{\\partial L}{\\partial \\hat{Y}} \\times \\frac{\\partial \\hat{Y}}{\\partial Z^{[2]}} = 0.46 \\times 1 = 0.46\n",
    "$$\n",
    "\n",
    "### 6.3 Gradients for $W^{[2]}$ and $b^{[2]}$\n",
    "\n",
    "**Weight gradient:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[2]}} = dZ^{[2]} \\cdot (A^{[1]})^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[2]}} = 0.46 \\times \\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.046 & 0.092 & 0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Bias gradient:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{[2]}} = dZ^{[2]} = 0.46\n",
    "$$\n",
    "\n",
    "### 6.4 Hidden Layer Gradient\n",
    "\n",
    "**Propagate error backwards:**\n",
    "\n",
    "$$\n",
    "dA^{[1]} = (W^{[2]})^T \\cdot dZ^{[2]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dA^{[1]} = \\begin{bmatrix}\n",
    "0.4 \\\\\n",
    "0.6 \\\\\n",
    "-0.2\n",
    "\\end{bmatrix} \\times 0.46 = \\begin{bmatrix}\n",
    "0.184 \\\\\n",
    "0.276 \\\\\n",
    "-0.092\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**ReLU derivative:**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}'(z) = \\begin{cases}\n",
    "1 & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Apply to our $Z^{[1]}$:**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}'(Z^{[1]}) = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} \\text{ (because } Z^{[1]} = \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix})\n",
    "$$\n",
    "\n",
    "**Element-wise multiplication:**\n",
    "\n",
    "$$\n",
    "dZ^{[1]} = dA^{[1]} \\odot \\text{ReLU}'(Z^{[1]}) = \\begin{bmatrix}\n",
    "0.184 \\\\\n",
    "0.276 \\\\\n",
    "-0.092\n",
    "\\end{bmatrix} \\odot \\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.184 \\\\\n",
    "0.276 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 6.5 Gradients for $W^{[1]}$ and $b^{[1]}$\n",
    "\n",
    "**Weight gradient:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[1]}} = dZ^{[1]} \\cdot X^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[1]}} = \\begin{bmatrix}\n",
    "0.184 \\\\\n",
    "0.276 \\\\\n",
    "0.0\n",
    "\\end{bmatrix} \\times [0.0] = \\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.0 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Bias gradient:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{[1]}} = dZ^{[1]} = \\begin{bmatrix}\n",
    "0.184 \\\\\n",
    "0.276 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Optimization (Weight Update)\n",
    "\n",
    "### 7.1 Gradient Descent\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "W = W - \\alpha \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "Inga:\n",
    "- $\\alpha$ = Learning rate (e.g., 0.01)\n",
    "- $\\frac{\\partial L}{\\partial W}$ = Gradient\n",
    "\n",
    "### 7.2 Update $W^{[2]}$ and $b^{[2]}$\n",
    "\n",
    "**Assume $\\alpha = 0.01$:**\n",
    "\n",
    "$$\n",
    "W^{[2]}_{new} = \\begin{bmatrix}\n",
    "0.4 & 0.6 & -0.2\n",
    "\\end{bmatrix} - 0.01 \\times \\begin{bmatrix}\n",
    "0.046 & 0.092 & 0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W^{[2]}_{new} = \\begin{bmatrix}\n",
    "0.39954 & 0.59908 & -0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{[2]}_{new} = 0.3 - 0.01 \\times 0.46 = 0.3 - 0.0046 = 0.2954\n",
    "$$\n",
    "\n",
    "### 7.3 Update $W^{[1]}$ and $b^{[1]}$\n",
    "\n",
    "$$\n",
    "W^{[1]}_{new} = \\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "-0.3 \\\\\n",
    "0.8\n",
    "\\end{bmatrix} - 0.01 \\times \\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.0 \\\\\n",
    "0.0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "-0.3 \\\\\n",
    "0.8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{[1]}_{new} = \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix} - 0.01 \\times \\begin{bmatrix}\n",
    "0.184 \\\\\n",
    "0.276 \\\\\n",
    "0.0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.09816 \\\\\n",
    "0.19724 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 7.4 Other Optimizers\n",
    "\n",
    "**1. Stochastic Gradient Descent (SGD):**\n",
    "- Oru sample use panni update pannuvom\n",
    "- Fast but noisy\n",
    "\n",
    "**2. Mini-Batch Gradient Descent:**\n",
    "- Small batch of samples use pannuvom\n",
    "- Balance between speed and stability\n",
    "\n",
    "**3. Adam Optimizer:**\n",
    "\n",
    "Most popular optimizer. Momentum and adaptive learning rate combine pannudum.\n",
    "\n",
    "**Formulas:**\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\quad \\text{(Momentum)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\quad \\text{(Adaptive learning rate)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\quad \\text{(Bias correction)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W = W - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "Inga:\n",
    "- $g_t$ = Current gradient\n",
    "- $\\beta_1$ = 0.9 (typical)\n",
    "- $\\beta_2$ = 0.999 (typical)\n",
    "- $\\epsilon$ = $10^{-8}$ (numerical stability ku)\n",
    "\n",
    "**4. RMSprop:**\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) g_t^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "W = W - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} g_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Training Loop\n",
    "\n",
    "**Complete Training Process:**\n",
    "\n",
    "```\n",
    "For each epoch (1 to 1000):\n",
    "    For each training sample:\n",
    "        1. Forward Propagation\n",
    "           - Calculate Z[1] = W[1] · X + b[1]\n",
    "           - Calculate A[1] = ReLU(Z[1])\n",
    "           - Calculate Z[2] = W[2] · A[1] + b[2]\n",
    "           - Calculate Ŷ = Z[2]\n",
    "        \n",
    "        2. Calculate Loss\n",
    "           - L = (1/2)(Y - Ŷ)²\n",
    "        \n",
    "        3. Backpropagation\n",
    "           - Calculate gradients for all weights and biases\n",
    "        \n",
    "        4. Update Weights\n",
    "           - W = W - α × gradient\n",
    "           - b = b - α × gradient\n",
    "    \n",
    "    If loss < threshold:\n",
    "        Stop training\n",
    "```\n",
    "\n",
    "**Convergence:**\n",
    "\n",
    "Multiple epochs ku apram, loss gradually reduce aagi, model accurate predictions pannudum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: CLASSIFICATION EXAMPLE\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Dataset: Student Pass/Fail Prediction**\n",
    "\n",
    "- **Input (X):** Study Hours = [1, 2, 3, 4, 5]\n",
    "- **Output (Y):** Pass/Fail = [0, 0, 1, 1, 1]\n",
    "\n",
    "Inga:\n",
    "- 0 = Fail\n",
    "- 1 = Pass\n",
    "\n",
    "**Goal:** Study hours kuduthu, student pass aaguvana illa fail aaguvana predict pannanum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Input Preparation\n",
    "\n",
    "**Raw Data:**\n",
    "\n",
    "```\n",
    "X = [1, 2, 3, 4, 5]\n",
    "Y = [0, 0, 1, 1, 1]\n",
    "```\n",
    "\n",
    "**Vector Format:**\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "4 \\\\\n",
    "5\n",
    "\\end{bmatrix}, \\quad Y = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Normalization:**\n",
    "\n",
    "$$\n",
    "X_{normalized} = \\frac{X - 1}{5 - 1} = \\frac{[0, 1, 2, 3, 4]}{4} = \\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.25 \\\\\n",
    "0.5 \\\\\n",
    "0.75 \\\\\n",
    "1.0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Network Architecture\n",
    "\n",
    "**Design:**\n",
    "- **Input Layer:** 1 neuron (Study hours)\n",
    "- **Hidden Layer:** 2 neurons\n",
    "- **Output Layer:** 1 neuron (Pass/Fail probability)\n",
    "\n",
    "```\n",
    "Input (1) → Hidden (2) → Output (1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Weight Initialization\n",
    "\n",
    "**Layer 1 (Input to Hidden):**\n",
    "\n",
    "$$\n",
    "W^{[1]} = \\begin{bmatrix}\n",
    "0.6 \\\\\n",
    "-0.4\n",
    "\\end{bmatrix}, \\quad b^{[1]} = \\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Layer 2 (Hidden to Output):**\n",
    "\n",
    "$$\n",
    "W^{[2]} = \\begin{bmatrix}\n",
    "0.5 & 0.7\n",
    "\\end{bmatrix}, \\quad b^{[2]} = 0.1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Forward Propagation\n",
    "\n",
    "### 4.1 First Sample ($x^{(1)} = 0.0$, $y^{(1)} = 0$)\n",
    "\n",
    "**Input to Hidden:**\n",
    "\n",
    "$$\n",
    "Z^{[1]} = W^{[1]} \\cdot X + b^{[1]} = \\begin{bmatrix}\n",
    "0.6 \\\\\n",
    "-0.4\n",
    "\\end{bmatrix} \\times 0.0 + \\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Activation (ReLU):**\n",
    "\n",
    "$$\n",
    "A^{[1]} = \\text{ReLU}(Z^{[1]}) = \\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Hidden to Output:**\n",
    "\n",
    "$$\n",
    "Z^{[2]} = W^{[2]} \\cdot A^{[1]} + b^{[2]} = \\begin{bmatrix}\n",
    "0.5 & 0.7\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "0.0\n",
    "\\end{bmatrix} + 0.1\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{[2]} = (0.5 \\times 0.2) + (0.7 \\times 0.0) + 0.1 = 0.1 + 0.0 + 0.1 = 0.2\n",
    "$$\n",
    "\n",
    "### 4.2 Output Activation (Sigmoid)\n",
    "\n",
    "**Classification ku Sigmoid use pannuvom:**\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Why Sigmoid?**\n",
    "- Output ah 0 to 1 range ku convert pannum\n",
    "- Probability ah interpret panna mudiyum\n",
    "- Binary classification ku perfect\n",
    "\n",
    "**Calculate:**\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\sigma(Z^{[2]}) = \\frac{1}{1 + e^{-0.2}} = \\frac{1}{1 + 0.8187} = \\frac{1}{1.8187} = 0.5498\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\hat{Y} = 0.5498$ means 54.98% probability of passing\n",
    "- Threshold 0.5 use pannina, 0.5498 > 0.5, so predict = 1 (Pass)\n",
    "- But actual Y = 0 (Fail), so prediction wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Loss Calculation\n",
    "\n",
    "### 5.1 Binary Cross-Entropy Loss\n",
    "\n",
    "Classification ku **Binary Cross-Entropy** use pannuvom:\n",
    "\n",
    "$$\n",
    "L = -[Y \\log(\\hat{Y}) + (1 - Y) \\log(1 - \\hat{Y})]\n",
    "$$\n",
    "\n",
    "**Why this formula?**\n",
    "- Y = 1 (Pass) aana: $L = -\\log(\\hat{Y})$\n",
    "  - $\\hat{Y}$ close to 1 aana, loss small\n",
    "  - $\\hat{Y}$ close to 0 aana, loss large\n",
    "- Y = 0 (Fail) aana: $L = -\\log(1 - \\hat{Y})$\n",
    "  - $\\hat{Y}$ close to 0 aana, loss small\n",
    "  - $\\hat{Y}$ close to 1 aana, loss large\n",
    "\n",
    "### 5.2 Calculate Loss\n",
    "\n",
    "**For first sample (Y = 0, $\\hat{Y}$ = 0.5498):**\n",
    "\n",
    "$$\n",
    "L = -[0 \\times \\log(0.5498) + (1 - 0) \\times \\log(1 - 0.5498)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = -[0 + 1 \\times \\log(0.4502)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = -\\log(0.4502) = -(-0.7985) = 0.7985\n",
    "$$\n",
    "\n",
    "**Total Loss (All Samples):**\n",
    "\n",
    "$$\n",
    "J = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Backpropagation\n",
    "\n",
    "### 6.1 Output Layer Gradient\n",
    "\n",
    "**Loss respect to output:**\n",
    "\n",
    "Binary cross-entropy + sigmoid ku derivative simple ah irukkum:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z^{[2]}} = \\hat{Y} - Y\n",
    "$$\n",
    "\n",
    "**Derivation:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{Y}} = -\\frac{Y}{\\hat{Y}} + \\frac{1-Y}{1-\\hat{Y}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{Y}}{\\partial Z^{[2]}} = \\sigma(Z^{[2]})(1 - \\sigma(Z^{[2]})) = \\hat{Y}(1 - \\hat{Y})\n",
    "$$\n",
    "\n",
    "**Chain rule apply pannina:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z^{[2]}} = \\frac{\\partial L}{\\partial \\hat{Y}} \\times \\frac{\\partial \\hat{Y}}{\\partial Z^{[2]}} = \\hat{Y} - Y\n",
    "$$\n",
    "\n",
    "**Calculate:**\n",
    "\n",
    "$$\n",
    "dZ^{[2]} = 0.5498 - 0 = 0.5498\n",
    "$$\n",
    "\n",
    "### 6.2 Gradients for $W^{[2]}$ and $b^{[2]}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[2]}} = dZ^{[2]} \\cdot (A^{[1]})^T = 0.5498 \\times \\begin{bmatrix}\n",
    "0.2 & 0.0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.1100 & 0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{[2]}} = dZ^{[2]} = 0.5498\n",
    "$$\n",
    "\n",
    "### 6.3 Hidden Layer Gradient\n",
    "\n",
    "$$\n",
    "dA^{[1]} = (W^{[2]})^T \\cdot dZ^{[2]} = \\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.7\n",
    "\\end{bmatrix} \\times 0.5498 = \\begin{bmatrix}\n",
    "0.2749 \\\\\n",
    "0.3849\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dZ^{[1]} = dA^{[1]} \\odot \\text{ReLU}'(Z^{[1]}) = \\begin{bmatrix}\n",
    "0.2749 \\\\\n",
    "0.3849\n",
    "\\end{bmatrix} \\odot \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.2749 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 6.4 Gradients for $W^{[1]}$ and $b^{[1]}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[1]}} = dZ^{[1]} \\cdot X^T = \\begin{bmatrix}\n",
    "0.2749 \\\\\n",
    "0.0\n",
    "\\end{bmatrix} \\times [0.0] = \\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b^{[1]}} = dZ^{[1]} = \\begin{bmatrix}\n",
    "0.2749 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Weight Update\n",
    "\n",
    "**Learning rate $\\alpha = 0.1$:**\n",
    "\n",
    "$$\n",
    "W^{[2]}_{new} = \\begin{bmatrix}\n",
    "0.5 & 0.7\n",
    "\\end{bmatrix} - 0.1 \\times \\begin{bmatrix}\n",
    "0.1100 & 0.0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.489 & 0.7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{[2]}_{new} = 0.1 - 0.1 \\times 0.5498 = 0.04502\n",
    "$$\n",
    "\n",
    "$$\n",
    "W^{[1]}_{new} = \\begin{bmatrix}\n",
    "0.6 \\\\\n",
    "-0.4\n",
    "\\end{bmatrix} - 0.1 \\times \\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.6 \\\\\n",
    "-0.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{[1]}_{new} = \\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix} - 0.1 \\times \\begin{bmatrix}\n",
    "0.2749 \\\\\n",
    "0.0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.1725 \\\\\n",
    "-0.1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Key Differences - Regression vs Classification\n",
    "\n",
    "| Aspect | Regression | Classification |\n",
    "|--------|-----------|----------------|\n",
    "| **Output Activation** | Linear (no activation) | Sigmoid (binary) / Softmax (multi-class) |\n",
    "| **Loss Function** | Mean Squared Error (MSE) | Binary Cross-Entropy / Categorical Cross-Entropy |\n",
    "| **Output Range** | Any real number | 0 to 1 (probability) |\n",
    "| **Prediction** | Continuous value | Class label (0 or 1) |\n",
    "| **Example** | House price, temperature | Pass/Fail, Cat/Dog |\n",
    "| **Evaluation Metric** | MAE, RMSE, R² | Accuracy, Precision, Recall, F1-score |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Complete Neural Network Flow\n",
    "\n",
    "### Chain Rule in Action\n",
    "\n",
    "**Forward Propagation Chain:**\n",
    "\n",
    "$$\n",
    "X \\xrightarrow{W^{[1]}, b^{[1]}} Z^{[1]} \\xrightarrow{\\text{ReLU}} A^{[1]} \\xrightarrow{W^{[2]}, b^{[2]}} Z^{[2]} \\xrightarrow{\\text{Activation}} \\hat{Y} \\xrightarrow{\\text{Loss}} L\n",
    "$$\n",
    "\n",
    "**Backpropagation Chain:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[2]}} = \\frac{\\partial L}{\\partial \\hat{Y}} \\times \\frac{\\partial \\hat{Y}}{\\partial Z^{[2]}} \\times \\frac{\\partial Z^{[2]}{\\partial W^{[2]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[1]}} = \\frac{\\partial L}{\\partial \\hat{Y}} \\times \\frac{\\partial \\hat{Y}}{\\partial Z^{[2]}} \\times \\frac{\\partial Z^{[2]}}{\\partial A^{[1]}} \\times \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\times \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}}\n",
    "$$\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Input Vectorization:** Data ah matrix format ku convert pannanum\n",
    "2. **Weight Initialization:** Random small values use pannanum\n",
    "3. **Forward Propagation:** Layer by layer multiply panni activate pannanum\n",
    "4. **Activation Functions:** Non-linearity add panna essential\n",
    "5. **Loss Calculation:** Prediction vs actual compare pannanum\n",
    "6. **Backpropagation:** Chain rule use panni gradients calculate pannanum\n",
    "7. **Optimization:** Gradients use panni weights update pannanum\n",
    "8. **Iteration:** Multiple epochs train panni model improve pannanum\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Neural networks la yellam **linear algebra** (matrices) and **calculus** (derivatives) base pannirukku. Chain rule dhan backpropagation oda heart!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
